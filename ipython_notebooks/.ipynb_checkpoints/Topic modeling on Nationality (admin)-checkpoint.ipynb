{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling on Nationality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic models are statistical models that aim to discover the 'hidden' thematic structure in a collection of documents, i.e. identify possible topics in our corpus. It is an interative process by nature, as it is crucial to determine the right number of topics. \n",
    "\n",
    "This notebook is organised as follows:\n",
    "\n",
    "* [Setup and dataset loading](#setup)\n",
    "* [Text Processing:](#text_process) Before feeding the data to a machine learning model, we need to convert it into numerical features.\n",
    "* [Topics Extraction Models:](#mod) We present two differents models from the sklearn library: NMF and LDA.\n",
    "* [Topics Visualisation with pyLDAvis](#viz)\n",
    "* [Topics Clustering:](#clust)  We try to understand how topics relate to each other.\n",
    "* [Further steps](#next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and dataset loading <a id=\"setup\" /> \n",
    "\n",
    "First of all, let's load the libraries that we'll use.\n",
    "\n",
    "**This notebook requires the installation of the [pyLDAvis](https://pyldavis.readthedocs.io/en/latest/readme.html#installation) package.**\n",
    "[See here for help with intalling python packages.](https://www.dataiku.com/learn/guide/code/python/install-python-packages.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named pyLDAvis.sklearn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f0e756363047>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNMF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named pyLDAvis.sklearn"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import warnings                         # Disable some warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "import dataiku\n",
    "from dataiku import pandasutils as pdu\n",
    "import pandas as pd,  seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation,NMF\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_limit = 10000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we do is now to load the dataset and identify possible text columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Take a handle on the dataset\n",
    "mydataset = dataiku.Dataset(\"Nationality\")\n",
    "\n",
    "# Load the first lines.\n",
    "# You can also load random samples, limit yourself to some columns, or only load\n",
    "# data matching some filters.\n",
    "#\n",
    "# Please refer to the Dataiku Python API documentation for more information\n",
    "df = mydataset.get_dataframe(limit = dataset_limit)\n",
    "\n",
    "df_orig = df.copy()\n",
    "\n",
    "# Get the column names\n",
    "numerical_columns = list(df.select_dtypes(include=[np.number]).columns)\n",
    "categorical_columns = list(df.select_dtypes(include=[object]).columns)\n",
    "date_columns = list(df.select_dtypes(include=['<M8[ns]']).columns)\n",
    "\n",
    "# Print a quick summary of what we just loaded\n",
    "print \"Loaded dataset\"\n",
    "print \"   Rows: %s\" % df.shape[0]\n",
    "print \"   Columns: %s (%s num, %s cat, %s date)\" % (df.shape[1], \n",
    "                                                    len(numerical_columns), len(categorical_columns),\n",
    "                                                    len(date_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, we suppose that the text of interest for which we want to extract topics is the first of the categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_col = categorical_columns[0]\n",
    "\n",
    "# Uncomment this if you want to take manual control over which variables is the text of interest\n",
    "#print df.columns\n",
    "#raw_text_col = \"text_normalized\"\n",
    "\n",
    "raw_text = df[raw_text_col]\n",
    "# Issue a warning if data contains NaNs\n",
    "if(raw_text.isnull().any()):\n",
    "    print('\\x1b[33mWARNING: Your text contains NaNs\\x1b[0m')\n",
    "    print('Please take care of them, the countVextorizer will not be able to fit your data if it contains empty values.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To test this notebook  on example data uncomment the following cell.**\n",
    "\n",
    "You can test this notebook on the 20newsgroups dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example on the 20newsgroups\n",
    "#from sklearn.datasets import fetch_20newsgroups\n",
    "#dataset = fetch_20newsgroups(shuffle=True, random_state=1,remove=('headers', 'footers', 'quotes'))\n",
    "#raw_text = dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing <a id=\"text_process\" /> \n",
    "\n",
    "We cannot directly feed the text to the Topics Extraction Algorithms. We first need to process the text in order to get numerical vectors. We achieve this by applying either a CountVectorizer() or a TfidfVectorizer(). For more information on those technics, please refer to thid [sklearn documentation](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with any text mining task, we first need to remove stop words that provide no useful information about topics. *sklearn* provides a default stop words list for english, but we can alway add to it any custom stop words : <a id=\"stop_words\" /a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = []\n",
    "#custom_stop_words = [u'did', u'good', u'right', u'said', u'does', u'way',u'edu', u'com', u'mail', u'thanks', u'post', u'address', u'university', u'email', u'soon', u'article',u'people', u'god', u'don', u'think', u'just', u'like', u'know', u'time', u'believe', u'say',u'don', u'just', u'think', u'probably', u'use', u'like', u'look', u'stuff', u'really', u'make', u'isn']\n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(custom_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer() on the text data <a id=\"tfidf\" /> \n",
    "\n",
    "We first initialise a CountVectorizer() object and then apply the fit_transform method to the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vectorizer = CountVectorizer(strip_accents = 'unicode',stop_words = stop_words,lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b', max_df = 0.85, min_df = 2)\n",
    "\n",
    "text_cnt = cnt_vectorizer.fit_transform(raw_text)\n",
    "\n",
    "print(text_cnt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer() on the text data <a id=\"tfidf\" /> \n",
    "\n",
    "We first initialise a TfidfVectorizer() object and then apply the fit_transform method to the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(strip_accents = 'unicode',stop_words = stop_words,lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b', max_df = 0.75, min_df = 0.02)\n",
    "\n",
    "text_tfidf = tfidf_vectorizer.fit_transform(raw_text)\n",
    "\n",
    "print(text_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will apply the topics extraction to `text_tidf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics Extraction Models <a id=\"mod\" /> \n",
    "\n",
    "There are two very popular models for topic modelling, both available in the sklearn library: \n",
    "\n",
    "* [NMF (Non-negative Matrix Factorization)](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization),\n",
    "* [LDA (Latent Dirichlet Allocation)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)\n",
    "\n",
    "Those two topic modeling algorithms infer topics from a collection of texts by viewing each document as a mixture of various topics. The only parameter we need to choose is the number of desired topics `n_topics`.  \n",
    "It is recommended to try different values for `n_topics` in order to find the most insightful topics. For that, we will show below different analyses (most frequent words per topics and heatmaps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics= 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this line for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_model = LatentDirichletAllocation(n_topics, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following line to try NMF instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topics_model = NMF(n_topics, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_model.fit(text_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Frequent Words per Topics\n",
    "An important way to assess the validity of our topic modelling is to directly look at the most frequent words in each topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_top_words = 10\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "def get_top_words_topic(topic_idx):\n",
    "    topic = topics_model.components_[topic_idx]\n",
    "   \n",
    "    print( [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]] )\n",
    "    \n",
    "for topic_idx, topic in enumerate(topics_model.components_):\n",
    "    print (\"Topic #%d:\" % topic_idx )\n",
    "    get_top_words_topic(topic_idx)\n",
    "    print (\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention to the words present, if some are very common you may want to go back to the [definition of custom stop words](#stop_words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naming the topics\n",
    "\n",
    "Thanks to the above analysis, we can try to name each topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_topic_name = {i: \"topic_\"+str(i) for i in xrange(n_topics)}\n",
    "#dict_topic_name = my_dict_topic_name #Define here your own name mapping and uncomment this !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 20newsgroup dataset, if you used the [suggested custom stop words](#stop_words) we suggest these 10 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_topic_name = {0: \"Posting\", 1: \"Driving\", 2: \"OS (Windows)\", 3: \"Past\", 4: \"Games\", 5: \"Sales\", 6: \"Misc\", 7: \"Christianity\", 8: \"Personal information\", 9: \"Government/Justice\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics Heatmaps\n",
    "\n",
    "Another visual helper to better understand the found topics is to look at the heatmap for the document-topic and topic-words matrices. This gives us the distribution of topics over the collection of documents and the distribution of words over the topics.  \n",
    "We start with the topic-word heatmap where the darker the color is the more the word is representative of the topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = pd.DataFrame(topics_model.components_.T)\n",
    "word_model.index = feature_names\n",
    "word_model.columns.name = 'topic'\n",
    "word_model['norm'] = (word_model).apply(lambda x: x.abs().max(),axis=1)\n",
    "word_model = word_model.sort_values(by='norm',ascending=0) # sort the matrix by the norm of row vector\n",
    "word_model.rename(columns = dict_topic_name, inplace = True) #naming topic\n",
    " \n",
    "del word_model['norm']\n",
    "\n",
    "plt.figure(figsize=(9,8))\n",
    "sns.heatmap(word_model[:10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now display the document-topic heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the document-topic matrix\n",
    "document_model = pd.DataFrame(topics_model.transform(text_tfidf))\n",
    "document_model.columns.name = 'topic'\n",
    "document_model.rename(columns = dict_topic_name, inplace = True) #naming topics\n",
    "\n",
    "plt.figure(figsize=(9,8))\n",
    "sns.heatmap(document_model.sort_index()[:10]) #we limit here to the first 10 texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic distribution over the corpus  \n",
    "We can look at how the topics are represented in the collection of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_proportion = document_model.sum()/document_model.sum().sum()\n",
    "topics_proportion.plot(kind = \"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each topic, we can investigate the documents the most representative for the given topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_documents_topics(topic_name, n_doc = 3, excerpt = True):\n",
    "    '''This returns the n_doc documents most representative of topic_name'''\n",
    "    \n",
    "    document_index = list(document_model[topic_name].sort_values(ascending = False).index)[:n_doc]\n",
    "    for order, i in enumerate(document_index):\n",
    "        print \"Text for the {}-th most representative document for topic {}:\\n\".format(order + 1,topic_name)\n",
    "        if excerpt:\n",
    "            print raw_text[i][:1000]\n",
    "        else:\n",
    "            print raw_text[i]\n",
    "        print \"\\n******\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 20newsgroup dataset, you can try this to get excerpts from the 3 most representative texts related to the Driving topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_documents_topics(\"topic_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics Visualization with pyLDAvis <a id=\"viz\">\n",
    "\n",
    "Thanks to the pyLDAvis package, we can easily visualise and interpret the topics that has been fit to our corpus of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(topics_model, text_tfidf, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics Clustering  <a id=\"clust\">  \n",
    "\n",
    "Once we have fitted topics on the text data, we can try to understand how they relate to one another: we achieve this by doing a hierachical clustering on the topics. We propose two methods, the first is based on a correlation table between topics, the second on a contigency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix between topics\n",
    "cor_matrix = np.corrcoef(document_model.iloc[:,:n_topics].values,rowvar=0)\n",
    "\n",
    "#Renaming of the index and columns\n",
    "cor_matrix = pd.DataFrame(cor_matrix)\n",
    "cor_matrix.rename(index = dict_topic_name, inplace = True)\n",
    "cor_matrix.rename(columns= dict_topic_name, inplace = True)\n",
    "\n",
    "sns.clustermap(cor_matrix, cmap=\"bone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contingency table on the binarized document-topic matrix\n",
    "document_bin_topic = (document_model.iloc[:,:n_topics] > 0.25).astype(int)\n",
    "contingency_matrix = np.dot(document_bin_topic.T.values, document_bin_topic.values )\n",
    "\n",
    "#Renaming of the index and columns\n",
    "contingency_matrix = pd.DataFrame(contingency_matrix)\n",
    "contingency_matrix.rename(index = dict_topic_name, inplace = True)\n",
    "contingency_matrix.rename(columns= dict_topic_name, inplace = True)\n",
    "\n",
    "sns.clustermap(contingency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further steps  <a id=\"next\">  \n",
    "\n",
    "Topics extraction is a vast subject and a notebook can only show so much. There still much thing we could do, here are some ideas:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Discard documents from noise topics\n",
    "The following helper function takes as argument the topics for which we wish to discard documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_doc(*topic_name):\n",
    "    \n",
    "    doc_max_topic = document_model.idxmax(axis = 1)\n",
    "    print \"Removing documents whose main topic are in \", topic_name\n",
    "    doc_max_topic_filtered = doc_max_topic[~doc_max_topic.isin(topic_name)]\n",
    "    return [raw_text[i] for i in doc_max_topic_filtered.index.tolist()]\n",
    "\n",
    "#E.g.: to remove documents whose main topic are topic_1 or topic_3, we would simply call remove_doc(\"topic_0\",\"topic_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 20newsgroup dataset, try this to remove text of topic \"Misc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_text_filtered = remove_doc(\"Misc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Scoring the topic model on new text\n",
    "Finally, we can score new text with our topic model as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = raw_text[:3] #Change this to the new text you'd like to score !\n",
    "\n",
    "tfidf_new_text = tfidf_vectorizer.transform(new_text)\n",
    "result = pd.DataFrame(topics_model.transform(tfidf_new_text), columns = [dict_topic_name[i] for i in xrange(n_topics)])\n",
    "sns.heatmap(result)"
   ]
  }
 ],
 "metadata": {
  "analyzedDataset": "Nationality",
  "creator": "admin",
  "customFields": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "tags": [],
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
