{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.10"
    },
    "analyzedDataset": "Conundrum_13_Data_prepared_scored",
    "creator": "admin",
    "tags": [],
    "customFields": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# High dimensional visualization on Conundrum_13_Data_prepared_scored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizing high dimensional data is a difficult problem and this notebook will help you apply t-SNE on your data. Although t-SNE is mathematically more complicated than other visualizations, it is also more powerful.\n",
        "\n",
        "The general idea for projecting high dimensional data into 2 dimensions is to preserve the original relative distances between points, but in the plane.\n",
        "\n",
        "A simple method called multidimensional scaling does the equivalent of putting springs between each of the points, whose strength is inversely proportional to the original distance. This will pull points together in the plane, and recover some of the original topology. However this method is not very efficient, and often falls into a local minimum that does not accurately represent the original topology.\n",
        "\n",
        "Another simple method is Sammon\u0027s mapping, in which we try harder to preserve the distances between nearby points than between those which are far apart. If two points are twice as close in the original space as two others, it is twice as important to maintain the distance between them. This method is better at revealing local, small-scale ordering.\n",
        "\n",
        "However, t-SNE does a very good job at revealing both the small scale and global topology of your high-dimensional data, and this is why we favor it. However, be aware that it can be harder to use correctly, so make sure to read the following instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A bit of theory\n",
        "\n",
        "The t-SNE algorithm comprises two main stages.\n",
        "First, t-SNE constructs a probability distribution over pairs of your high-dimensional points, so that similar ones have a high probability of being picked, whilst dissimilar points have an extremely small probability of being picked.\n",
        "\n",
        "Second, t-SNE defines another probability distribution over the points in the two-dimensional plane, and it minimizes the Kullback–Leibler divergence (a measure of the difference between two probability distributions) between the high-dimensional and two-dimensional distributions, with respect to the locations of the points in the map.\n",
        "\n",
        "More details at https://lvdmaaten.github.io/tsne/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recommendation\n",
        "\n",
        "Before applying t-SNE, it is highly recommended to use another dimensionality reduction method (e.g. PCA) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high.\n",
        "\n",
        "More details at http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
        "\n",
        "In this notebook, we will display t-SNE results for both the raw data, the rescaled data and data on which PCA was applied."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outline\n",
        "\n",
        "* [Setup and loading the data](#setup)\n",
        "* [Preprocessing of the data](#preprocessing)\n",
        "* [t-Distributed Stochastic Neighbor Embedding (aka t-SNE)](#tsne)\n",
        "\n",
        "\u003ccenter\u003e\u003cstrong\u003eSelect Cell \u003e Run All to execute the whole analysis\u003c/strong\u003e\u003c/center\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and dataset loading \u003ca id\u003d\"setup\" /\u003e \n",
        "\n",
        "First of all, let\u0027s load the libraries that we\u0027ll use"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "%pylab inline\n",
        "import dataiku                               # Access to Dataiku datasets\n",
        "import pandas as pd, numpy as np             # Data manipulation\n",
        "from sklearn.decomposition import PCA        # PCA capability\n",
        "from sklearn.manifold import TSNE            # Import TSNE capability from scikit\n",
        "from matplotlib import pyplot as plt         # Graphing\n",
        "from collections import defaultdict, Counter # Utils\n",
        "import warnings                              # Disable some warnings\n",
        "warnings.filterwarnings(\"ignore\",category\u003dDeprecationWarning)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first thing we do is now to load the dataset and put aside the three main types of columns:\n",
        "\n",
        "* Numerics\n",
        "* Categorical\n",
        "* Dates\n",
        "\n",
        "Since analyzing PCA requires to have the data in memory, we are only going to load a sample of the data. Modify the following cell to change the size of the sample.\n",
        "\n",
        "By default, date features are not kept. Modify the following cell to change that.\n",
        "\n",
        "Also, by default, categorical columns are not included in the t-SNE data (preventing the creation of very sparse data). Modify the following cell to enable dummification.\n",
        "We also use a categorical column for coloring if possible."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "# Change sample size here\n",
        "dataset_limit \u003d 10000\n",
        "# Toggle to keep dates in your dataset\n",
        "keep_dates \u003d False\n",
        "# Toggle to dummify categorical features\n",
        "dummify_categories \u003d False"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load a DSS dataset as a Pandas dataframe"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Take a handle on the dataset\n",
        "mydataset \u003d dataiku.Dataset(\"Conundrum_13_Data_prepared_scored\")\n",
        "\n",
        "# Load the first lines.\n",
        "# You can also load random samples, limit yourself to some columns, or only load\n",
        "# data matching some filters.\n",
        "#\n",
        "# Please refer to the Dataiku Python API documentation for more information\n",
        "df \u003d mydataset.get_dataframe(limit \u003d dataset_limit)\n",
        "\n",
        "df_orig \u003d df.copy()\n",
        "\n",
        "# Get the column names\n",
        "numerical_columns \u003d list(df.select_dtypes(include\u003d[np.number]).columns)\n",
        "categorical_columns \u003d list(df.select_dtypes(include\u003d[object]).columns)\n",
        "date_columns \u003d list(df.select_dtypes(include\u003d[\u0027\u003cM8[ns]\u0027]).columns)\n",
        "\n",
        "# Print a quick summary of what we just loaded\n",
        "print \"Loaded dataset\"\n",
        "print \"   Rows: %s\" % df.shape[0]\n",
        "print \"   Columns: %s (%s num, %s cat, %s date)\" % (df.shape[1], \n",
        "                                                    len(numerical_columns), len(categorical_columns),\n",
        "                                                    len(date_columns))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing of the data \u003ca id\u003d\"preprocessing\" /\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Handle dates\n",
        "\n",
        "Keep the dates as features if requested by the user"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "columns_to_drop \u003d []\n",
        "\n",
        "if keep_dates:\n",
        "    df[date_columns] \u003d df[date_columns].astype(int)*1e-9\n",
        "else:\n",
        "    columns_to_drop.extend(date_columns)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Categorical coloring\n",
        "You can choose which color to use here, or we will select one automatically."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "import matplotlib.cm as cm\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "def find_color_column(df, categorical_columns):\n",
        "    color_column \u003d []\n",
        "    if len(categorical_columns) \u003d\u003d 0:\n",
        "        return color_column\n",
        "\n",
        "    else:\n",
        "        color_column \u003d categorical_columns[0]\n",
        "        \n",
        "        for col in categorical_columns:\n",
        "            if df[col].nunique() \u003e 3 and df[col].nunique() \u003c 10:\n",
        "                color_column \u003d col\n",
        "                break;\n",
        "    return color_column\n",
        "\n",
        "def generate_color_replacements(color_column):\n",
        "    colors \u003d cm.rainbow(np.linspace(0, 1, df[color_column].nunique()))\n",
        "    return {key: value for (key, value) in  zip(df[color_column].unique().tolist(), colors)}"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "color_column \u003d find_color_column(df, categorical_columns)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Change the value here to use a custom column for coloring your t-SNE."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "#color_column \u003d \u0027my_color_column\u0027\n",
        "print \"We will use column \u0027{}\u0027 for coloring.\".format(color_column)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We generate the list of colors and the legend"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "patches \u003d []\n",
        "\n",
        "if color_column:\n",
        "    cat_replacements \u003d generate_color_replacements(color_column)    \n",
        "    coloring_list \u003d df[color_column].map(cat_replacements)\n",
        "\n",
        "    for couple in cat_replacements:\n",
        "        patch \u003d mpatches.Patch(color\u003dcat_replacements[couple], label\u003dcouple.decode(\u0027utf8\u0027))\n",
        "        patches.append(patch)\n",
        "else:\n",
        "    coloring_list \u003d np.zeros(df.shape[0])"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Additional preparation\n",
        "\n",
        "Get rid of the columns that contain too many unique values"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "DROP_LIMIT_ABS \u003d 200\n",
        "CAT_DROP_LIMIT_RATIO \u003d 0.5\n",
        "for feature in categorical_columns:\n",
        "    nu \u003d df[feature].nunique()\n",
        "    \n",
        "    if nu \u003e DROP_LIMIT_ABS or nu \u003e CAT_DROP_LIMIT_RATIO*df.shape[0]:\n",
        "        print \"Dropping feature %s with %s values\" % (feature, nu)\n",
        "        columns_to_drop.append(feature)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then need to impute missing values"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Use mean for numerical features\n",
        "for feature in numerical_columns:\n",
        "    v \u003d df[feature].mean()\n",
        "    if np.isnan(v):\n",
        "        v \u003d 0\n",
        "    print \"Filling %s with %s\" % (feature, v)\n",
        "    df[feature] \u003d df[feature].fillna(v)\n",
        "    \n",
        "# Use mode for categorical features\n",
        "for feature in categorical_columns:\n",
        "    v \u003d df[feature].value_counts().index[0]\n",
        "    df[feature] \u003d df[feature].fillna(v)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Drop the columns"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "print \"Dropping the following columns: %s\" % columns_to_drop\n",
        "df \u003d df.drop(columns_to_drop, axis\u003d1)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dummy encoding\n",
        "For all categorical features, we are going to \"dummy-encode\" them (also sometimes called one-hot encoding).\n",
        "\n",
        "Basically, a categorical feature is replaced by one column per value. Each created value contains 0 or 1 depending on whether the original value was the one of the column."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# For categorical variables with more than that many values, we only keep the most frequent ones\n",
        "LIMIT_DUMMIES \u003d 100\n",
        "\n",
        "# Only keep the top 100 values\n",
        "def select_dummy_values(train, features):\n",
        "    dummy_values \u003d {}\n",
        "    for feature in features:\n",
        "        values \u003d [\n",
        "            value\n",
        "            for (value, _) in Counter(train[feature]).most_common(LIMIT_DUMMIES)\n",
        "        ]\n",
        "        dummy_values[feature] \u003d values\n",
        "    return dummy_values\n",
        "\n",
        "DUMMY_VALUES \u003d select_dummy_values(df, [x for x in categorical_columns if not x in columns_to_drop])\n",
        "\n",
        "\n",
        "def dummy_encode_dataframe(df):\n",
        "    for (feature, dummy_values) in DUMMY_VALUES.items():\n",
        "        for dummy_value in dummy_values:\n",
        "            dummy_name \u003d u\u0027%s_value_%s\u0027 % (feature, dummy_value.decode(\u0027utf-8\u0027))\n",
        "            df[dummy_name] \u003d (df[feature] \u003d\u003d dummy_value).astype(float)\n",
        "        del df[feature]\n",
        "        print \u0027Dummy-encoded feature %s\u0027 % feature\n",
        "\n",
        "dummy_encode_dataframe(df)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rescaling\n",
        "We use standard rescaling for applying t-SNE on the rescaled data."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "X \u003d df.values\n",
        "    \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss \u003d StandardScaler().fit(X)\n",
        "X_std \u003d ss.transform(X)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PCA application\n",
        "Using principal component analysis is optional, but will help in the case of very high-dimensional data.\n",
        "\n",
        "You can set the variance to keep (90% by default) so that you end up with less dimensions, which should help the t-SNE algorithm."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "sklearn_pca \u003d PCA()\n",
        "Y_sklearn \u003d sklearn_pca.fit_transform(X_std)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "VARIANCE_TO_KEEP \u003d 0.9\n",
        "keep_recommend \u003d [sklearn_pca.explained_variance_ratio_[:y].sum()\u003eVARIANCE_TO_KEEP for y in range(1,sklearn_pca.n_components_+1)].count(False)\n",
        "print \"Number of components to keep to retain %s%% of the variance:\" % (100*VARIANCE_TO_KEEP), keep_recommend, \"out of the original\", sklearn_pca.n_components_"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "sklearn_pca_final \u003d PCA(n_components\u003dkeep_recommend)\n",
        "Y_sklearn_final \u003d sklearn_pca_final.fit_transform(X_std)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## t-Distributed Stochastic Neighbor Embedding (aka t-SNE) \u003ca id\u003d\"tsne\" /\u003e"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Create data from 1000 first rows of the rescaled data, with PCA applied or not\n",
        "# You can change the number of rows to use here\n",
        "n\u003d1000\n",
        "X_df \u003d X[:n]\n",
        "X_rescaled \u003d X_std[:n]\n",
        "X_PCA \u003d Y_sklearn_final[:n]\n",
        "coloring_list \u003d coloring_list[:n]"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute and visualize the t-SNE projection of the raw data"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "tsne \u003d TSNE(n_components\u003d2, random_state\u003d0)\n",
        "tsne_data \u003d tsne.fit_transform(X_df)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": false
      },
      "source": [
        "plt.figure(figsize\u003d(13,9))\n",
        "plt.scatter(tsne_data[:,0],tsne_data[:,1], c\u003dcoloring_list)\n",
        "plt.legend(handles\u003dpatches)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute and visualize the t-SNE projection of the rescaled data"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "tsne_data_rescaled \u003d tsne.fit_transform(X_rescaled)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "plt.figure(figsize\u003d(13,9))\n",
        "plt.scatter(tsne_data_rescaled[:,0],tsne_data_rescaled[:,1], c\u003dcoloring_list)\n",
        "plt.legend(handles\u003dpatches)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute and visualize the t-SNE projection of the PCA projected data"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "tsne_data_PCA \u003d tsne.fit_transform(X_PCA)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "plt.figure(figsize\u003d(13,9))\n",
        "plt.scatter(tsne_data_PCA[:,0],tsne_data_PCA[:,1], c\u003dcoloring_list)\n",
        "plt.legend(handles\u003dpatches)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Advanced t-SNE usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When using t-SNE as an exploratory technique, it is important to understand the most common pitfalls. You can read [Wattenberg, et al. “How to Use t-SNE Effectively”, Distill, 2016](http://distill.pub/2016/misread-tsne/) for more information.\n",
        "\n",
        "We will use a list of different perplexity values to help you see if structure in your data is consistent. You can try with the raw data (X_df), rescaled data (X_std) or PCA transformed data (X_PCA)."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "perplexity_list \u003d [5, 10, 30, 50, 100]\n",
        "tsne_perp \u003d [ TSNE(n_components\u003d2, random_state\u003d0, perplexity\u003dperp) for perp in perplexity_list]\n",
        "tsne_perp_data \u003d [ tsne.fit_transform(X_df) for tsne in tsne_perp]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "plt.figure(figsize\u003d(15,15))\n",
        "for i in range(len(perplexity_list)):\n",
        "    plt.subplot(3,2, i+1)\n",
        "    plt.title(\"Perplexity \" + str(perplexity_list[i]))\n",
        "    plt.scatter(tsne_perp_data[i][:,0],tsne_perp_data[i][:,1], c\u003dcoloring_list)\n",
        "plt.subplot(3,2, i+2)\n",
        "plt.axis(\u0027off\u0027)\n",
        "plt.legend(handles\u003dpatches, loc\u003d\u0027upper left\u0027)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": true
      },
      "source": [],
      "outputs": []
    }
  ]
}